# byDataMining

## 实验要求

1.    文本类别数：>=10类；
2.    训练集文档数：>=500000篇；每类平均50000篇。
3.    测试集文档数：>=500000篇；每类平均50000篇。
4.    分组完成实验，组员数量<=3，个人实现可以获得实验加分。

利用分类算法实现对文本的数据挖掘，主要包括：
1.    语料库的构建，主要包括利用爬虫收集Web文档等；
2.    语料库的数据预处理，包括文档建模，如去噪，分词，建立数据字典， 使用词袋模型或主题模型表达文档等；
注：使用主题模型，如LDA可以获得实验加分；
3.    选择分类算法（朴素贝叶斯（必做）、SVM/其他等），训练文本分类器， 理解所选的分类算法的建模原理、实现过程和相关参数的含义；
4.    对测试集的文本进行分类
5.    对测试集的分类结果利用正确率和召回率进行分析评价：计算每类正确 率、召回率，计算总体正确率和召回率，以及F-score。

### 1 实验目的

通过对数据仓库与数据挖掘课程的学习，了解一些数据挖掘的基本算法，在整体上认识和加深对数据挖掘的理解，并掌握以下技术：
1，掌握数据预处理的方法，对训练集进行采集；
2，掌握文本建模的方法，对语料库档进行建模；
3，掌握分类算法的原理，基于有监督机器学习方法训练文本；
4，利用学习的文本分类器，对未知文本进行判别；
5，掌握评价分类器性能的估方法。

### 2 实验环境

由自编写爬虫程序爬取中国新闻网的十类新闻，训练集加上测试集共一百万篇文档作为本实验的语料库。	
实验平台：Windows10，PyCharm, Python3

### 3 主要设计思想

#### 3.1 自建语料库介绍

本实验由自编写爬虫程序爬取中国新闻网的十类新闻，训练集加上测试集共一百万篇文档作为本实验的语料库。其中类别包括：汽车，财经，文化，法制，军事，社会，证券，台湾，体育，娱乐共十个分类。
每个分类爬取文章10万篇，其中5万作为训练集，5万作为测试集。数据集大小达到了实验的百万数据要求。
利用爬虫程序爬取文章分为两步。1）从新闻网主页利用BFS广度优先遍历所有文章链接。2）将这些Url下对应的文章处理成文本数据保存在本地。

#### 3.2 结巴分词系统

结巴汉语分词系统,主要功能包括中文分词；词性标注；命名实体识别；用户词典功能；是Python分词的重要插件。详细参考：https://github.com/fxsjy/jieba
结巴分词主要算法原理：基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)，采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。
详细参考：https://github.com/fxsjy/jieba （下载地址），网页内同时也包含了基本的使用方法。

### 4 实验原理

#### 4.1 停用词处理，数据降维，词典的构建

在进行特征词提取之前，对文档中进行了去停用词处理。停用词来源为网上下载的通用停用词文档。将文档中出现但是在停用词表中也出现的词进行剔除。
然后采用PCA降维方式来提取特征词汇，并将其纳入词典。主要思想是：将原始数据以线性形式映射到维度互不相关的子空间。主要就是寻找方差最大的不相关维度。数据的最大方差给出了数据的最重要信息。
这里借用了sklearn.decomposition中的降维方法，将词典(词袋模型)中的特征词维度降至了8000维。
提取出来的特征词，表达了训练集的特征，用词袋模型来存储，即对每个特征词进行唯一的数字编号，并将词和数字的一一对应关系保留下来供后期使用。本实验通过建立词典的方式表示每一篇文档，词汇通过统计的词频获取，对应的数字由词频表示。

#### 4.2 分类算法的选择

朴素贝叶斯算法。
基本原理：
1，条件概率： 表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为： 。
2,贝叶斯公式： 。
3，P(B)叫做先验概率，本次实验中测试集的文本共分为10类，所以在不清楚类别的情况相，每一篇文档随机分类正确的概率为1/10(也可以称之为先验概率为1/10)。
4，贝叶斯分类的基本原理：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

线性SVM算法
基本原理：
SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。
LinearSVC还有以下特点：
基于liblinear库实现
有多种惩罚参数和损失函数可供选择
训练集实例数量大（大于1万）时也可以很好地进行归一化
既支持稠密输入矩阵也支持稀疏输入矩阵
多分类问题采用one-vs-rest方法实现

#### 4.3性能评估方法
 
计算每一类的准确率、召回率，平均准确率、召回率以及训练时间，最后进行综合分析。

### 5 实验过程

#### 5.1 数据处理及清洗

对每个txt文件进行分词(只选取名词)处理。 截图略。

#### 5.2 数据降维，提取特征词

这里借用了sklearn中的特征词提取方法，进行降维处理，保存为本实验的特征词典. 截图略。

#### 5.3 统计词频，逆文档频率，计算TFIDF及各分类每个维度的条件概率

1）统计词频，计算TF矩阵。
每篇文章统计对应的tf行，即统计在特征词典里面的词的出现次数。

2）统计逆文档频率IDF。
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
这里进行了平滑处理，防止分母为零。

3）计算TFIDF矩阵。
TFIDF为TF*IDF，表示对当前词频进行加权处理。

4）计算各分类每个维度的条件概率
各分类每个维度的条件概率为当前已知分类下出现各个特征词的概率。对tf矩阵中同属一类别的行向量进行相加，然后计算条件概率。
总体的计算代码如下：

截图略。

####  5.4 利用朴素贝叶斯原理进行分类预测

最后，利用贝叶斯分类器对文档进行分类。
对于每篇文档，在每个分类下的概率只需要考虑P(X|yi)*P(yi)。
这里进行了零概率处理。对于每个分类下的条件概率，如果有些文档没有出现某些特征词，则该项概率为零，这样由于每个特征词的相互独立性，会导致最后概率为零。
为了防止这一情况，将每一特征结果加1取对数处理。 截图略。

####  5.5 结果展示

截图略。
